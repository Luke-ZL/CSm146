\documentclass[11pt]{article}
\usepackage{course}
\usepackage{upquote}
\usepackage{minted}
\fvset{frame=single}




\begin{document}


\ctitle{2}{Linear Classification and Logistic Regression}{Oct. 30}
\author{}
\date{}
\maketitle
\vspace{-0.75in}

\vspace{-11pt}
 
\blfootnote{Parts of this assignment are adapted from course material by Andrew Ng (Stanford), Jenna Wiens (UMich) and Jessica Wu (Harvey Mudd).}

\ifsoln
\else
\section*{Submission}
\begin{itemize}
\item 
Submit your solutions electronically on the course Gradescope site as PDF files.
\item If you plan to typeset your solutions, please use the LaTeX solution template. If you must submit scanned handwritten solutions, please use a black pen on blank white paper and a high-quality scanner app.
\end{itemize}
\fi

\ifnotsolution{\newpage}
\section{Linear Model \problemworth{10}}
Design (specify $w$) for a two-input linear model (with an additional bias or offset term) that computes the following boolean functions. Assume $True=1$ and $False=-1$. If a valid linear model exists, show that it is not unique by designing another valid linear model (with a different hyperplane, not simply through normalization). If no such linear model exists, state why.
\begin{itemize}
	\item [(a)] AND \problemworth{5}
	\item [(b)] XOR \problemworth{5}
\end{itemize}
\solution{}

\section{Logistic Regression \problemworth{10}}

Consider the logistic regression model $h_{\vect{w}}(\vect{x}) = \sigma(\vect{w}^T \vect{x})=\frac{1}{1+ exp(-\vect{w}^Tx)}$, the objective loss function that we minimize is:
\begin{equation*}
J(\vect{w}) = 
  - \sum_{n=1}^{N}{
  \left[ y_{n} \log h_{\vect{w}}\left(\vect{x}_{n}\right) 
  + \left(1 - y_{n}\right) \log \left(1 - h_{\vect{w}}\left(\vect{x}_{n}\right)\right) \right]}
\end{equation*}


\begin{enumerate}

\item  Find the partial derivatives $\frac{\partial J}{\partial w_j}$.  \problemworth{10}
 
\solution{


}

\end{enumerate}



\section{Locally Weighted Linear Regression \problemworth{10}}
Consider a linear regression problem in which we want to ``weight'' different training instances differently because some of the instances are more important than others. Specifically, suppose we want to minimize
\begin{equation*}
J(w_0,w_1) =  \sum_{n=1}^{N}{\alpha_{n} \left( w_0 + w_1 x_{n,1} - y_{n}\right)^2}.
\end{equation*}
Here $\alpha_n > 0 $. In class, we worked out what happens for the case where all the weights (the $\alpha_{n}$'s) are the same. In this problem, we will generalize some of those ideas to the weighted setting. 

\begin{enumerate}

\item \itemworth{6} Calculate the gradient by computing the partial derivatives of $J$ with respect to each of the parameters $(w_0, w_1)$.

\solution{

}
\item \itemworth{4} Set each partial derivatives to $0$ and solve for $w_0$ and $w_1$ to obtain values of $(w_0,w_1)$ that minimize $J$. 

\solution{
}

\end{enumerate}

%\newpage
\section{Understanding Linear Separability \problemworth{25}}

\begin{definition}[Linear Program]
A {\em linear program} can be stated as follows:

Let 
$A$ be an $m \times n$ real-valued matrix,
$\vec{b} \in \mathbb{R}^m$, and $\vec{c} \in \mathbb{R}^n$.
Find a $\vec{t} \in \mathbb{R}^n$, that minimizes the linear function
\begin{eqnarray*}
  & & z(\vec{t}) = \vec{c}~^T \vec{t} \\
\textrm{subject to} & & A \vec{t} \geq \vec{b}
\end{eqnarray*}
\end{definition}

In the linear programming terminology, $\vec{c}$ is often referred
to as the {\em cost vector} and $z(\vec{t})$ is referred to as the {\em objective
function}.\footnote{Note that you don't need
to really know how to solve a linear program since we will use Matlab to obtain the
solution (although you may wish to brush up on Linear Algebra). 
Also see the appendix for more information on linear programming.}
We can use this framework to define the problem of learning a linear
discriminant function.\footnote{This discussion closely
  parallels the linear programming representation found in 
  {\em Pattern Classification}, by Duda, Hart, and Stork.}

\textbf{The Learning Problem:}\footnote{Note that the notation used in the
Learning Problem is
{\bf unrelated} to the one used in the Linear Program definition. You may want to
figure out the correspondence.} \hspace{2mm}
Let $\vec{x_1}, \vec{x_2}, \ldots, \vec{x_m}$ 
represent $m$ samples, where each sample $\vec{x_i}\in \mathbb{R}^n$ is an $n$-dimensional
vector, and $\vec{y} \in \{-1, 1\}^m$ is an $m \times 1$
vector representing the respective labels of each of the $m$ samples. Let
$\vec{w} \in \mathbb{R}^n$ be an $n \times 1$ vector representing the weights of the
linear discriminant function, and $\theta$ be the threshold value. 

We {\em predict} $\vec{x_i}$ to be a {\em positive} example if
$\vec{w}^T \vec{x_i} + b \geq 0$. On the other hand, we {\em predict}
$\vec{x_i}$ to be a {\em negative} example if $\vec{w}^T \vec{x_i} + b < 0$.

We hope that the learned linear function can separate the data set.  
That is,
\begin{equation}
\label{eq:separable}
y_i = \begin{cases}
 1 & \mbox{if } \vec{w}^T \vec{x_i} + b \ge 0 \\
-1 & \mbox{if } \vec{w}^T \vec{x_i} + b < 0. \\
\end{cases}
\end{equation}

In order to find a good linear separator, we propose the following linear program:
\begin{eqnarray}
  \min & & \delta  \nonumber \\
 \textrm{subject to } & & y_i(\vec{w}^T \vec{x_i} + b ) \geq 1 - \delta, \qquad \forall (\vec{x_i},y_i) \in D \nonumber  \\
  & & \delta \geq 0  \label{eq:lin_prog_discriminant_bound}
\end{eqnarray}
where $D$ is the data set of all training examples.

\ifnotsolution{\newpage}

\begin{enumerate}
\item \itemworth{5} A data set $D=\{(\vec{x_i},y_i)\}_{i=1}^m$ that satisfies 
      condition (1) above is called
      {\em linearly separable}. 
      Show that if $D$ is linearly separable, there is an optimal solution to the linear program (2) with $\delta=0$

\solution{ }      
      
\item \itemworth{5} Now show that there is an optimal solution with $\delta=0$, then $D$ is linearly separable.

\solution{ }

\item \itemworth{5} 
What can we say about the linear separability of the data set if there exists a hyperplane that satisfies condition (2) with $\delta>0$?

\solution{}

\item \itemworth{5}
An alternative LP formulation to (2) may be \newline
\begin{eqnarray}
  \min & & \delta  \nonumber \\
 \textrm{subject to } & & y_i(\vec{w}^T \vec{x_i} + b) \geq -\delta, \qquad \forall (\vec{x_i},y_i) \in D \nonumber  \\
  & & \delta \geq 0 \nonumber
\end{eqnarray}
Find the optimal solution to this formulation (independent of $D$) to illustrate the issue with such a
formulation.

\solution{}


\item \itemworth{5} Let $\vec{x_1} \in \mathbb{R}^n$, $\vec{x_1}^T =
\begin{bmatrix}
  1 & 1 & 1
\end{bmatrix}$ and $y_1 = 1$. 
Let
$\vec{x_2} \in \mathbb{R}^n$, $\vec{x_2}^T =
\begin{bmatrix}
  -1 & -1 & -1
\end{bmatrix}$ and $y_2 = -1$.
The data set $D$ is defined as
\begin{equation*}
    D = \{ (\vec{x_1},y_1), (\vec{x_2},y_2)\}.
\end{equation*}
Consider the formulation in (2)
applied to $D$. What are possible optimal solutions?
\end{enumerate}
\solution{}


\newpage
\section{Implementation: Polynomial Regression \problemworth{45}}

In this exercise, you will work through linear and polynomial regression. Our data consists of inputs $x_n \in \mathbb{R}$ and outputs $y_n \in \mathbb{R}, n \in \{1,\ldots,N\}$, which are related through a target function $y = f(x)$. Your goal is to learn a linear predictor $h_{\vect{w}}(x)$ that best approximates $f(x)$. But this time, rather than using \verb|scikit-learn|, we will further open the ``black-box'', and you will implement the regression model!

\rule{\textwidth}{1pt}
code and data
\begin{itemize}[nolistsep]
\item code : \verb|regression.py|
\item data : \verb|regression_train.csv|, \verb|regression_test.csv|
\end{itemize}
\vspace{-\baselineskip}
\rule{\textwidth}{1pt}

This is likely the first time that many of you are working with \verb|numpy| and matrix operations within a programming environment. For the uninitiated, you may find it useful to work through a \verb|numpy| tutorial first.\footnote{Try out \verb~SciPy~'s tutorial (\url{http://wiki.scipy.org/Tentative_NumPy_Tutorial}), or use your favorite search engine to find an alternative. Those familiar with Matlab may find the ``Numpy for Matlab Users'' documentation (\url{http://wiki.scipy.org/NumPy_for_Matlab_Users}) more helpful.} Here are some things to keep in mind as you complete this problem:
\begin{itemize}[nosep]
\item If you are seeing many errors at runtime, inspect your matrix operations to make sure that you are adding and multiplying matrices of compatible dimensions. Printing the dimensions of variables with the \verb|X.shape| command will help you debug.
\item When working with \verb|numpy| arrays, remember that \verb|numpy| interprets the \verb|*| operator as element-wise multiplication. This is a common source of size incompatibility errors. If you want matrix multiplication, you need to use the \verb|dot| function in Python. For example, \verb|A*B| does element-wise multiplication while \verb|dot(A,B)| does a matrix multiply.
\item Be careful when handling \verb|numpy| vectors (rank-1 arrays): the vector shapes $1 \times N$, $N \times 1$, and $N$ are all different things. For these dimensions, we follow the the conventions of \verb|scikit-learn|'s \verb|LinearRegression| class\footnote{\url{http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html}}. Most importantly, unless otherwise indicated (in the code documentation), both column and row vectors are rank-1 arrays of shape $N$, not rank-2 arrays of shape $N \times 1$ or shape $1 \times N$.
\end{itemize}



\vspace{10pt} {\large \textbf{Visualization} \problemworth{5}}

As we learned last week, it is often useful to understand the data through visualizations. For this data set, you can use a scatter plot to visualize the data since it has only two properties to plot ($x$ and $y$).

\begin{enumerate}

\item \itemworth{3} Visualize the training and test data using the \verb|plot_data(...)| function. What do you observe? For example, can you make an educated guess on the effectiveness of linear regression in predicting the data?


\solution{

}

\end{enumerate}



\newpage
\vspace{10pt} {\large \textbf{Linear Regression} \problemworth{25}}

Recall that linear regression attempts to minimize the objective function
\begin{equation*}
J(\vect{w}) =  \sum_{n=1}^{N}{\left(h_{\vect{w}}(\vect{x}_{n}) - y_{n}\right)^2}.
\end{equation*}

In this problem, we will use the matrix-vector form where
\begin{equation*}
\vect{y} =\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{N}
\end{pmatrix},
\quad \quad \quad
\matr{X} = \begin{pmatrix}
\vect{x}_{1}^T \\
\vect{x}_{2}^T  \\
\vdots  \\
\vect{x}_{N}^T  \\
\end{pmatrix},
\quad \quad \quad
\vect{w} = \begin{pmatrix}
w_0\\
w_1\\
w_2\\
\vdots\\
w_D
\end{pmatrix}
\end{equation*}
and each instance $\vect{x}_{n} = \bigl( 1, x_{n,1}, \ldots, x_{n,D} \bigr)^T$. 

In this instance, the number of input features $D=1$. 

Rather than working with this fully generalized, multivariate case, let us start by considering a simple linear regression model:
\begin{equation*}
h_{\vect{w}}(\vect{x}) = \vect{w}^T \vect{x} = w_0 + w_1 x_1
\end{equation*}


\verb|regression.py| contains the skeleton code for the class \verb|PolynomialRegression|. Objects of this class can be instantiated as \verb|model = PolynomialRegression (m)| where $m$ is the degree of the polynomial feature vector where the feature vector for instance $n$, $\bigl( 1, x_{n,1}, x_{n,1}^2, \ldots, x_{n,1}^{m} \bigr)^T$. Setting $m=1$ instantiates an object where the feature vector for instance $n$, $\bigl( 1, x_{n,1}\bigr)^T$. 

\begin{enumerate}[resume]
\item \itemworth{2} Note that to take into account the intercept term ($w_0$), we can add an additional ``feature'' to each instance and set it to one, e.g. $x_{i,0} = 1$. This is equivalent to adding an additional first column to $\matr{X}$ and setting it to all ones.

Modify \verb|PolynomialRegression.generate_polynomial_features(...)| to create the matrix $\matr{X}$ for a simple linear model.
\solution{
 }

\item \itemworth{2} Before tackling the harder problem of training the regression model, complete \\\verb|PolynomialRegression.predict(...)| to predict $\vect{y}$ from $\matr{X}$ and $\vect{w}$.
\solution{
 }

\item \itemworth{7} One way to solve linear regression is through gradient descent (GD).

Recall that the parameters of our model are the $w_j$ values. These are the values we will adjust to minimize $J(\vect{w})$. In gradient descent, each
iteration performs the update
\begin{equation*}
w_j \leftarrow w_j - 2  \alpha \sum_{n=1}^{N} \left( h_{\vect{w}}(\vect{x}_{n}) - y_{n} \right) x_{n,j} \textrm{\quad (simultaneously update $w_j$ for all $j$)}.
\end{equation*}
With each step of gradient descent, we expect our updated parameters $w_j$ to come closer to the parameters that will achieve the lowest value of $J(\vect{w})$.

\begin{itemize}

\item \itemworth{1} As we perform gradient descent, it is helpful to monitor the convergence by computing the cost, \emph{i.e.}, the value of the objective function $J$. Complete \verb|PolynomialRegression.cost(...)| to calculate $J(\vect{w})$.


If you have implemented everything correctly, then the following code snippet should return $40.234$.
\begin{minted}{python}
train_data = load_data('regression_train.csv')
model = PolynomialRegression()
model.coef_ = np.zeros(2)
model.cost(train_data.X, train_data.y)
\end{minted}

\solution{

}

\item \itemworth{3} Next, implement the gradient descent step in \verb|PolynomialRegression.fit_GD(...)|.  The loop structure has been written for you, and you only need to supply the updates to $\vect{w}$ and the new predictions $\hat{y} = h_{\vect{w}}(\vect{x})$ within each iteration.

We will use the following specifications for the gradient descent algorithm:
\begin{itemize}
\item We run the algorithm for $10,000$ iterations.
\item We terminate the algorithm ealier if the value of the objective function is unchanged across consecutive iterations.
\item We will use a fixed step size. 
\end{itemize}

\solution{
}

\item \itemworth{3} So far, you have used a default learning rate (or step size) of $\eta = 0.01$. Try different $\eta = 10^{-4}, 10^{-3}, 10^{-2}, 0.0407$, and make a table of the coefficients, number of iterations until convergence (this number will be $10,000$ if the algorithm did not converge in a smaller number of iterations) and the final value of the objective function. How do the coefficients compare? How quickly does each algorithm converge?
\solution{

}

\end{itemize}

\item \itemworth{4} In class, we learned that the closed-form solution to linear regression is
\begin{equation*}
\vect{w} = (\matr{X}^T \matr{X})^{-1} \matr{X}^T \vect{y}.
\end{equation*}
Using this formula, you will get an exact solution in one calculation: there is no ``loop until convergence'' like in gradient descent.

\begin{itemize}
\item \itemworth{2} Implement the closed-form solution \verb|PolynomialRegression.fit(...)|.
\solution{
}
\item \itemworth{2} What is the closed-form solution? How do the coefficients and the cost compare to those obtained by GD? How quickly does the algorithm run compared to GD?
\solution{
}
\end{itemize}

\item \itemworth{1} Finally, set a learning rate $\eta$ for GD that is a function of $k$ (the number of iterations) (use $\eta_k = \tfrac{1}{1+k}$) and converges to the same solution yielded by the closed-form optimization (minus possible rounding errors). Update \verb|PolynomialRegression.fit_GD(...)| with your proposed learning rate. How long does it take the algorithm to converge with your proposed learning rate?
\solution{

}

\end{enumerate}



\vspace{10pt} {\large \textbf{Polynomial Regression}\problemworth{15}}

Now let us consider the more complicated case of polynomial regression, where our hypothesis is
\begin{equation*}
h_{\vect{w}}(\vect{x}) = \vect{w}^T \phi(\vect{x}) = w_0 + w_1 x + w_2 x^2 + \ldots + w^m x^m.
\end{equation*}

\begin{enumerate}[resume]

\item \itemworth{1} 
Recall that polynomial regression can be considered as an extension of linear regression in which we replace our input matrix $\matr{X}$ with
\begin{equation*}
\matr{\Phi} = \begin{pmatrix}
\phi(x_{1})^T  \\
\phi(x_{2})^T  \\
\vdots  \\
\phi(x_{N})^T \\
\end{pmatrix},
\end{equation*}
where $\phi(x)$ is a function such that $\phi_j(x) = x^j$ for $j = 0, \ldots, m$.

Update \verb|PolynomialRegression.generate_polynomial_features(...)| to create an $m+1$ dimensional feature vector for each instance.
\solution{
}

\item \itemworth{4} Given $N$ training instances, it is always possible to obtain a ``perfect fit'' (a fit in which all the data points are exactly predicted) by setting the degree of the regression to $N-1$. Of course, we would expect such a fit to generalize poorly. In the remainder of this problem, you will investigate the problem of overfitting as a function of the degree of the polynomial, $m$. To measure overfitting, we will use the Root-Mean-Square (RMS) error, defined as 
\begin{equation*}
E_{RMS} = \sqrt{J(\vect{w}) / N},
\end{equation*}
where $N$ is the number of instances.\footnote{Note that the RMSE as defined is a biased estimator. To obtain an unbiased estimator, we would have to divide by $n-k$, where $k$ is the number of parameters fitted (including the constant), so here, $k = m+1$.}

Why do you think we might prefer RMSE as a metric over $J(\vect{w})$?
\solution{
 }

Implement \verb|PolynomialRegression.rms_error(...)|.
\solution{
 }

\item \itemworth{5} For $m = 0, \ldots, 10$, use the closed-form solver to determine the best-fit polynomial regression model on the training data, and with this model, calculate the RMSE on both the training data and the test data. Generate a plot depicting how RMSE varies with model complexity (polynomial degree) -- you should generate a single plot with both training and test error, and include this plot in your writeup. Which degree polynomial would you say best fits the data? Was there evidence of under/overfitting the data? Use your plot to justify your answer.
\solution{

}

\end{enumerate}

\end{document}